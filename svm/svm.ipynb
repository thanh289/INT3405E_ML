{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba7c157",
   "metadata": {},
   "source": [
    "# Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efac6e3",
   "metadata": {},
   "source": [
    "The loss function of SVM is as follows:\n",
    "\\begin{align}\n",
    "L(w) = \\frac{1}{2}||w||^2 + C\\Sigma_{i=1}^{N}max(0, 1-y_i(w^Tx_i-b))\n",
    "\\end{align}\n",
    "\n",
    "Normally, we'll use +b instead of -b, but for some reasons, the code given used -b, so yeah just keep it. As the result, the gradient descent for bias will be a little bit different.\n",
    "\n",
    "The derivative of max component:\n",
    "\n",
    "\n",
    "* if $1-y_i(wx_i -b) >0$, max function equals $1-y_i(wx_i +b)$ and the derivate equals $-y_ix_i$\n",
    "* if $1-y_i(wx_i -b) <=0$, max function equals 0 and the derivative equals 0\n",
    "\n",
    "\n",
    "\n",
    "Update parameter with gradient descent is as follows:\n",
    "\\begin{align}\n",
    "w = w - lr(w-C\\Sigma_{i=1}^{N}\\delta_i y_i x_i)\n",
    "\\end{align}\n",
    "\n",
    "In particular:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = \\mathbf{w} - y_i \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = y_i\n",
    "$$\n",
    "\n",
    "→ cập nhật:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta (\\mathbf{w} - y_i \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b - \\eta (y_i) = b - \\eta y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ebd6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9bc9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Margin SVM\n",
    "\n",
    "class HardMarginSVM:\n",
    "    def __init__(self, lr=1e-3, n_iterations=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            for i in range(n_samples):\n",
    "                # We can use matrix product @, but for 1d vector, we can use dot product, is the same still\n",
    "                condition = y[i] * (np.dot(self.weights, X[i]) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    # haven't known that before\n",
    "                    # regularization, allow the border to be larger\n",
    "                    self.weights -= self.lr * self.weights \n",
    "                else:\n",
    "                    self.weights -= self.lr * (self.weights - np.dot(y[i], X[i]))\n",
    "                    self.bias -= self.lr * y[i]\n",
    "\n",
    "    # return the linear\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) - self.bias\n",
    "        # np.sign: return +1 if it > 0, else ...\n",
    "        return np.sign(linear_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38cc445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: [ 1.  1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Create example data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
    "y = np.array([1, 1, 1, -1, -1, -1])\n",
    "\n",
    "# Training with hard margin SVM\n",
    "svm = HardMarginSVM()\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "predictions = svm.predict(X)\n",
    "print(\"Predict:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c53d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Margin SVM\n",
    "class SoftMarginSVM:\n",
    "    def __init__(self, lr=1e-3, C=1.0, n_iterations=1000):\n",
    "        self.lr = lr\n",
    "        self.C = C\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            for i in range(n_samples):\n",
    "                condition = y[i] * (np.dot(self.weights, X[i]) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.lr * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.lr * (2 * self.C * self.weights - np.dot(y[i], X[i]))\n",
    "                    self.bias -= self.lr * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) - self.bias\n",
    "        return np.sign(linear_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e5a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: [ 1.  1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
    "y = np.array([1, 1, 1, -1, -1, -1])\n",
    "\n",
    "svm = SoftMarginSVM(C=0.1)\n",
    "svm.fit(X, y)\n",
    "\n",
    "predictions = svm.predict(X)\n",
    "print(\"Predict:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
