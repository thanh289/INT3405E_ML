{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e22167",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "708138bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (569, 30)\n",
      "y (569,)\n",
      "Test accuracy:  0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print('X', X.shape)\n",
    "print('y', y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "model_overfit = LogisticRegression(max_iter=10000)\n",
    "model_overfit.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_overfit.predict(X_test)\n",
    "accuracy_overfit = accuracy_score(y_pred, y_test)\n",
    "print(\"Test accuracy: \", accuracy_overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc9510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accracy with selected features: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# SelectKBest: select K best features by a criteria\n",
    "# f_classif: ANOVA F-test - calculate the correlation between features and output\n",
    "\n",
    "k_best = SelectKBest(f_classif, k=10)\n",
    "X_train_selected = k_best.fit_transform(X_train, y_train)\n",
    "X_test_selected = k_best.transform(X_test)\n",
    "\n",
    "model_selected = LogisticRegression(max_iter=10000)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = model_selected.predict(X_test_selected)\n",
    "accruacy_selected = accuracy_score(y_pred, y_test)\n",
    "print(\"Test accracy with selected features:\" , accruacy_selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9af63",
   "metadata": {},
   "source": [
    "## Using Chi-square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4219e8",
   "metadata": {},
   "source": [
    "![Chi-Square](https://cdn1.byjus.com/wp-content/uploads/2020/10/Chi-Square-Test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c86366",
   "metadata": {},
   "source": [
    "The theory is pretty long, you can search in internet, but for the code, all you have to know is that the bigger the chi-square number, the bigger the correlation between this feature and the ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd3521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features have been selected: ['area error' 'worst area' 'mean area' 'perimeter error' 'worst perimeter'\n",
      " 'worst radius' 'mean perimeter' 'mean radius' 'worst texture'\n",
      " 'worst concavity']\n",
      "Accuracy with features selected by chi-square: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "chi2_values = []\n",
    "\n",
    "# Calculate the chi2 score for each feature\n",
    "for feature_idx in range(X.shape[1]):\n",
    "    observed_values = np.column_stack((X[:, feature_idx], y))\n",
    "    chi2, _, _, _ = chi2_contingency(observed_values)\n",
    "    chi2_values.append(chi2)\n",
    "\n",
    "# Sort the value (we want the largest chi2)\n",
    "# But return an arrray of the index\n",
    "sorted_feature_index = np.argsort(chi2_values)[::-1]\n",
    "\n",
    "# Filter\n",
    "num_selected_features = 10\n",
    "seleted_feature_index = sorted_feature_index[:num_selected_features]\n",
    "\n",
    "selected_feature_names = np.array(data.feature_names)[seleted_feature_index]\n",
    "print(\"Features have been selected:\", selected_feature_names)\n",
    "\n",
    "\n",
    "# The reset just to fit the model\n",
    "X_train_selected = X_train[:, seleted_feature_index]\n",
    "X_test_selected = X_test[:, seleted_feature_index]\n",
    "\n",
    "model_selected = LogisticRegression(max_iter=10000)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = model_selected.predict(X_test_selected)\n",
    "selected_accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(\"Accuracy with features selected by chi-square:\", selected_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1faf48",
   "metadata": {},
   "source": [
    "## Using F-score (F-value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb59b60",
   "metadata": {},
   "source": [
    "It's the same if we use f_classif library from sklearn (we have coded above), but ye, here's what the same way, result will also be the same, of course\n",
    "\n",
    "- In the code, **F-score is used to rank features**.\n",
    "- Higher F → more important feature for classification.\n",
    "- If you want to know more about it, then read the part below **(optional)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa475d",
   "metadata": {},
   "source": [
    "### F-score theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a105b",
   "metadata": {},
   "source": [
    "- F-test checks:\n",
    "> \"Do the means of a feature differ significantly across different classes?\"\n",
    "\n",
    "- If **class means are very different**, the feature is likely **important**.\n",
    "- If **class means are similar**, the feature is less informative.\n",
    "\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "- $ K $ classes\n",
    "- $ N $ total samples\n",
    "- Class $ k $ has $ n_k $ samples\n",
    "- $ x_{ki} $ is the value of feature $ x $ for sample $ i $ in class $ k $\n",
    "- $ \\bar{x}_k $ = mean of class $ k $\n",
    "- $ \\bar{x} $ = overall mean of the feature\n",
    "\n",
    "\n",
    "$F = \\frac{MS_{between}}{MS_{within}}$\n",
    "\n",
    "Where:\n",
    "- **Between-class mean square (MS_between)**\n",
    "$MS_{between} = \\frac{1}{K-1} \\sum_{k=1}^{K} n_k (\\bar{x}_k - \\bar{x})^2$\n",
    "\n",
    "- **Within-class mean square (MS_within)**\n",
    "$MS_{within} = \\frac{1}{N-K} \\sum_{k=1}^{K} \\sum_{i=1}^{n_k} (x_{ki} - \\bar{x}_k)^2$\n",
    "\n",
    "\n",
    "- $ (\\bar{x}_k - \\bar{x})^2$ → how far class mean is from overall mean\n",
    "- $ (x_{ki} - \\bar{x}_k)^2 $→ variance within the class\n",
    "- **High F-score** → feature discriminates well between classes\n",
    "- **Low F-score** → feature is less informative\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c415aa8",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "Suppose we have **1 feature** and **2 classes**:\n",
    "\n",
    "| Class | Feature values \\(x\\) |\n",
    "|-------|---------------------|\n",
    "| 0     | [1, 2, 3]           |\n",
    "| 1     | [7, 8, 9]           |\n",
    "\n",
    "**Step 1: Compute class means and overall mean**\n",
    "\n",
    "$\\bar{x}_0 = 2, \\quad \\bar{x}_1 = 8, \\quad \\bar{x} = 5$\n",
    "\n",
    "**Step 2: Between-class sum of squares (SSB)**\n",
    "\n",
    "$SS_B = n_0(\\bar{x}_0 - \\bar{x})^2 + n_1(\\bar{x}_1 - \\bar{x})^2 = 3(2-5)^2 + 3(8-5)^2 = 54$\n",
    "\n",
    "**Step 3: Within-class sum of squares (SSW)**\n",
    "\n",
    "$SS_W = \\sum (x_{ki} - \\bar{x}_k)^2 = 4$\n",
    "\n",
    "**Step 4: Mean squares**\n",
    "\n",
    "$MS_B = SS_B / (K-1) = 54 / 1 = 54$\n",
    "$MS_W = SS_W / (N-K) = 4 / (6-2) = 1$\n",
    "\n",
    "**Step 5: F-score**\n",
    "\n",
    "$F = MS_B / MS_W = 54 / 1 = 54$\n",
    "\n",
    "So the result we have is large F-score → feature strongly discriminates between the two classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "574d9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by f-score: ['worst concave points' 'worst perimeter' 'mean concave points'\n",
      " 'worst radius' 'mean perimeter' 'worst area' 'mean radius' 'mean area'\n",
      " 'mean concavity' 'worst concavity']\n",
      "Accuracy with features selected by f-score: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Split the data by class\n",
    "X_class0 = X[y==0]\n",
    "X_class1 = X[y==1]\n",
    "\n",
    "f_scores = []\n",
    "for feature_idx in range(X.shape[1]):\n",
    "    f_score, _ = f_oneway(X_class0[:, feature_idx], X_class1[:, feature_idx])\n",
    "    f_scores.append(f_score)\n",
    "\n",
    "# This part is pretty the same with chi2\n",
    "sorted_feature_index = np.argsort(f_scores)[::-1]\n",
    "\n",
    "num_selected_features = 10\n",
    "selected_feature_index  = sorted_feature_index[:num_selected_features]\n",
    "\n",
    "selected_feature_names = np.array(data.feature_names)[selected_feature_index]\n",
    "print(\"Features selected by f-score:\", selected_feature_names)\n",
    "\n",
    "# ¬_¬\n",
    "X_train_selected = X_train[:, selected_feature_index]\n",
    "X_test_selected = X_test[:, selected_feature_index]\n",
    "\n",
    "model_selected = LogisticRegression(max_iter=10000)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = model_selected.predict(X_test_selected)\n",
    "selected_accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(\"Accuracy with features selected by f-score:\", selected_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb4364",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d579ed",
   "metadata": {},
   "source": [
    "By combining multiple models (weak learners) -> create a stronger model (strong leaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15d3ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data  \n",
    "y = data.target  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac6ca3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980b33f",
   "metadata": {},
   "source": [
    "## Bagging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215919e2",
   "metadata": {},
   "source": [
    "Train multiple independent models by boostraping the data, then take the average or take a poll to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a09089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82ea23ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baggin (KNN) accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# We will create n_estimators KNN models\n",
    "n_estimators = 100\n",
    "boostrap_samples = 50\n",
    "\n",
    "weak_models = []\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "    boostrap_index = np.random.choice(X_train.shape[0], boostrap_samples, replace=True)\n",
    "    X_boostrap = X_train[boostrap_index]\n",
    "    y_boostrap = y_train[boostrap_index]\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_boostrap, y_boostrap)\n",
    "    weak_models.append(knn)\n",
    "\n",
    "predictions = np.zeros((y_test.shape[0], n_estimators))\n",
    "for i, knn in enumerate(weak_models):\n",
    "    predictions[:, i] = knn.predict(X_test)\n",
    "\n",
    "ensemble_predictions = np.round(np.mean(predictions, axis=1))\n",
    "\n",
    "accuracy = accuracy_score(ensemble_predictions, y_test)\n",
    "print(f\"Baggin (KNN) accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d05f3",
   "metadata": {},
   "source": [
    "# Boostring Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2d727",
   "metadata": {},
   "source": [
    "Training models continuously, while each model focus on the sample that the previous model predict wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb07e53",
   "metadata": {},
   "source": [
    "\n",
    "We start by assigning **equal weights** to all training samples.\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1}{N}\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of training samples.\n",
    "At each iteration $t$:\n",
    "\n",
    "1. Train a **Decision Stump** (a one-level Decision Tree) using the current sample weights.\n",
    "2. Let the model be $h_t(x)$.\n",
    "\n",
    "$$\n",
    "\\text{error}_t = \\frac{\\sum_i w_i [y_i \\neq h_t(x_i)]}{\\sum_i w_i}\n",
    "$$\n",
    "\n",
    "This represents how often the model misclassifies samples, taking their weights into account.\n",
    "\n",
    "\n",
    "The weight of the weak learner is:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\text{error}_t}{\\text{error}_t} \\right)\n",
    "$$\n",
    "\n",
    "- If the model performs well → low error → large $ \\alpha_t $\n",
    "- If the model performs poorly (error > 0.5) → negative $ \\alpha_t $\n",
    "\n",
    "\n",
    "After each iteration, increase the weight of the **misclassified** samples so the next learner focuses on them.\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i \\cdot e^{-\\alpha_t y_i h_t(x_i)}\n",
    "$$\n",
    "\n",
    "Then normalize:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow \\frac{w_i}{\\sum_j w_j}\n",
    "$$\n",
    "\n",
    "- If $ y_i = h_t(x_i) $ → correctly classified → weight decreases  \n",
    "- If $ y_i \\neq h_t(x_i) $ → misclassified → weight increases\n",
    "\n",
    "\n",
    "The final strong classifier is a weighted majority vote:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)\n",
    "$$\n",
    "\n",
    "- Each model votes with strength proportional to its accuracy  ($\\alpha_t$)\n",
    "- The sign of the weighted sum gives the final class label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dc03d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Boosting (KNN) 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 50\n",
    "\n",
    "estimators = []\n",
    "\n",
    "weights = np.ones(len(X_train)) / len(X_train)\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "    tree = DecisionTreeClassifier(max_depth=1)\n",
    "    tree.fit(X_train, y_train, sample_weight=weights)\n",
    "\n",
    "    y_pred = tree.predict(X_train)\n",
    "\n",
    "    error = np.sum(weights * (y_pred != y_train)) / np.sum(weights)\n",
    "    tree_weight = 0.5 * np.log((1-error) / error)\n",
    "    weights = weights * np.exp(-tree_weight * y_train * y_pred)\n",
    "\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    estimators.append((tree, tree_weight))\n",
    "\n",
    "y_pred_ensemble = np.zeros_like(y_test, dtype=float)\n",
    "for tree, tree_weight in estimators:\n",
    "    y_pred_tree = tree.predict(X_test)\n",
    "    y_pred_ensemble += tree_weight * y_pred_tree\n",
    "\n",
    "y_pred_ensemble = np.sign(y_pred_ensemble)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(\"Accuracy of Boosting (KNN)\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8207ebb",
   "metadata": {},
   "source": [
    "## Stacking Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f588cac",
   "metadata": {},
   "source": [
    "This one is pretty simple so i won't explain anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29a3258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacking Ensemble: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Base models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "knn_model.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "stacked_predictions = np.column_stack((rf_pred, knn_pred, lr_pred))\n",
    "\n",
    "# Meta learner\n",
    "meta_learner = LogisticRegression()\n",
    "meta_learner.fit(stacked_predictions, y_test)\n",
    "final_predictions = meta_learner.predict(stacked_predictions)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Accuracy of Stacking Ensemble: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
